{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cars & Trucks.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "IqLbv7Pk5jvA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is an Image Classification Analysis, in which I am using images of cars and trucks to train my data and later make a prediction whether the given image is of a car or a truck.\n",
        "\n",
        "The data I have used have seperate folders for Train and Test. Images of cars and trucks are further stored in seperate folders.\n",
        "I am using Tensorflow library for creating neural network.\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "VC7wtGDZLMmi",
        "colab_type": "code",
        "outputId": "b7f1ba6a-ffeb-4a5c-8c21-c8d40afe7a6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install tflearn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.13.0rc1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.9)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.6.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.14.0rc0,>=1.13.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.13.0rc0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.6)\n",
            "Requirement already satisfied: tensorboard<1.13.0,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (40.8.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.13.0,>=1.12.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock>=2.0.0->tensorflow-estimator<1.14.0rc0,>=1.13.0rc0->tensorflow) (5.1.2)\n",
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.6/dist-packages (0.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.14.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tflearn) (1.11.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from tflearn) (4.0.0)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->tflearn) (0.46)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p6HoO2ra716L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "My data is present on google drive and accessing the data using PyDrive."
      ]
    },
    {
      "metadata": {
        "id": "ffz9K78pMKnS",
        "colab_type": "code",
        "outputId": "7a38b1ac-ae9d-40c5-b001-afe111f12628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyDrive\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/e0/0e64788e5dd58ce2d6934549676243dc69d982f198524be9b99e9c2a4fd5/PyDrive-1.3.1.tar.gz (987kB)\n",
            "\r\u001b[K    1% |▎                               | 10kB 14.7MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 3.3MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 4.7MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 3.1MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 3.8MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 4.5MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 5.2MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 5.8MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 6.5MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 5.0MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 5.1MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 6.9MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 6.9MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 12.2MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 12.4MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 12.4MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 12.1MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 12.1MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 12.1MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 44.4MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 15.0MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 14.9MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 15.1MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 15.3MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 15.2MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 14.6MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 15.1MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 15.2MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 15.0MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 15.3MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 50.3MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 52.4MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 53.9MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 48.3MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 48.9MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 57.8MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 58.9MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 59.5MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 20.0MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 19.5MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 19.1MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 19.1MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 18.8MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 19.0MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 19.0MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 19.0MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 19.0MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 18.9MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 52.2MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 51.6MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 53.9MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 53.7MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 55.6MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 61.2MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 17.8MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 17.3MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 17.2MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 17.0MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 16.9MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 17.3MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 17.4MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 17.5MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 17.5MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 17.5MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 44.7MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 47.6MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 48.6MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 50.2MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 51.0MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 51.7MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 51.2MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 51.0MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 51.6MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 51.6MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 73.0MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 73.5MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 72.8MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 74.4MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 75.0MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 73.4MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 76.0MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 77.1MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 75.5MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 63.0MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 61.2MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 62.4MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 62.5MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 61.1MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 61.4MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 62.1MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 62.2MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 62.2MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 62.7MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 76.0MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 78.4MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 76.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 23.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.6.7)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.11.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.11.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.5)\n",
            "Building wheels for collected packages: PyDrive\n",
            "  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/fa/d2/9a/d3b6b506c2da98289e5d417215ce34b696db856643bad779f4\n",
            "Successfully built PyDrive\n",
            "Installing collected packages: PyDrive\n",
            "Successfully installed PyDrive-1.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sb1NlGabLZMr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yW0VZS21MF5Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gR_hJlNnZZty",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "\n",
        "# providing size for resizing of images\n",
        "img_size = 50\n",
        "\n",
        "# Learning rate \n",
        "lr = 1e-3\n",
        "\n",
        "# Giving Model Name having 6 Convonutional layers\n",
        "model_name = 'cars vs truck-{}-{}.model'.format(lr,'6conv-basic')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QDxG3GKP9Sft",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Loading the train data of cars and trucks images"
      ]
    },
    {
      "metadata": {
        "id": "WomgfMWfNm-0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download = drive.CreateFile({'id': '1DeI5xZYA52WO2k5IiwHFSrMez5g2Tlj7'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfLg9cxoO1YJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1DeI5xZYA52WO2k5IiwHFSrMez5g2Tlj7' in parents\"}).GetList()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZFaiVEyvO3Ch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fetching every image from the folder and converting it to grayscale\n",
        "\n",
        "# for every image of truck, introducing label [0,1] and [1,0] for car image.\n",
        "\n",
        "# loading Truck train data\n",
        "\n",
        "train = []\n",
        "for f in file_list:\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(f['title'])\n",
        "  img = cv2.resize(cv2.imread(f['title'],cv2.IMREAD_GRAYSCALE),(img_size,img_size))\n",
        "  train.append([np.array(img),np.array([0,1])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZShbXkXaelxq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fqo51hqnbld_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download = drive.CreateFile({'id': '1i9dJA5z-qhTDjHhCSMibSUMfgsC_uiXQ'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MU9kekcieLeE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1i9dJA5z-qhTDjHhCSMibSUMfgsC_uiXQ' in parents\"}).GetList()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Mpo3jJkeYq8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Car's train Data\n",
        "\n",
        "for f in file_list:\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(f['title'])\n",
        "  img = cv2.resize(cv2.imread(f['title'],cv2.IMREAD_GRAYSCALE),(img_size,img_size))\n",
        "  train.append([np.array(img),np.array([1,0])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YUTPZs_uezXt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0ErPae_De4kD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download = drive.CreateFile({'id': '1ItW67KSt44Vsk55JUf4q0MFXwIY5UYSU'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wP0kn2H5e6k_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1ItW67KSt44Vsk55JUf4q0MFXwIY5UYSU' in parents\"}).GetList()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2XLMOb9gfTv2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading Test data in similar manner\n",
        "\n",
        "test = []\n",
        "for f in file_list:\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(f['title'])\n",
        "  img = cv2.resize(cv2.imread(f['title'],cv2.IMREAD_GRAYSCALE),(img_size,img_size))\n",
        "  test.append([np.array(img),np.array([1,0])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LGE-eGyifqZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JQc2XbFFftJ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "download = drive.CreateFile({'id': '1zJpTtQyPmuRSkOqzRPMzvGSzbNzhDkwy'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZibG5alBf19V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_list = drive.ListFile({'q': \"'1zJpTtQyPmuRSkOqzRPMzvGSzbNzhDkwy' in parents\"}).GetList()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z_sX7-Nzf8Al",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for f in file_list:\n",
        "  f_ = drive.CreateFile({'id': f['id']})\n",
        "  f_.GetContentFile(f['title'])\n",
        "  img = cv2.resize(cv2.imread(f['title'],cv2.IMREAD_GRAYSCALE),(img_size,img_size))\n",
        "  test.append([np.array(img),np.array([0,1])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BkmVNVRZgAxZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Shuffeling Data for random occurance of cars and trucks\n",
        "\n",
        "shuffle(train)\n",
        "shuffle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ls9EkWmf4kOa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# For clearing the graph\n",
        "\n",
        "from tensorflow import reset_default_graph\n",
        "reset_default_graph()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2xFc-HC6_b4q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating tflearn model with 6 layers. The more layers we introduce, we lower the value of Loss, but it should be causing any overfitting.So we should have balance between these."
      ]
    },
    {
      "metadata": {
        "id": "qQGSsIfwgHJO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "5b2b97df-1698-4c3e-ec12-bdf0c13a7950"
      },
      "cell_type": "code",
      "source": [
        "import tflearn\n",
        "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
        "from tflearn.layers.core import input_data, dropout, fully_connected\n",
        "from tflearn.layers.estimator import regression\n",
        "\n",
        "convnet = input_data(shape=[None,img_size,img_size,1],name='input')\n",
        "\n",
        "convnet = conv_2d(convnet,32,2,activation='relu')\n",
        "convnet = max_pool_2d(convnet,2)\n",
        "\n",
        "convnet = conv_2d(convnet,64,2,activation='relu')\n",
        "convnet = max_pool_2d(convnet,2)\n",
        "\n",
        "convnet = conv_2d(convnet,32,2,activation='relu')\n",
        "convnet = max_pool_2d(convnet,2)\n",
        "\n",
        "convnet = conv_2d(convnet,64,2,activation='relu')\n",
        "convnet = max_pool_2d(convnet,2)\n",
        "\n",
        "convnet = conv_2d(convnet,32,2,activation='relu')\n",
        "convnet = max_pool_2d(convnet,2)\n",
        "\n",
        "convnet = conv_2d(convnet,64,2,activation='relu')\n",
        "convnet = max_pool_2d(convnet,2)\n",
        "\n",
        "convnet = fully_connected(convnet,1024,activation='relu')\n",
        "convnet = dropout(convnet,0.8)\n",
        "\n",
        "convnet = fully_connected(convnet,2,activation='softmax')\n",
        "convnet = regression(convnet,optimizer = 'adam', learning_rate = lr, loss = 'categorical_crossentropy', name='targets')\n",
        "\n",
        "model = tflearn.DNN(convnet, tensorboard_dir = \"log\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/initializations.py:119: UniformUnitScaling.__init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/layers/core.py:239: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tflearn/objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OwOg77xHANvN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Splitting data and labels "
      ]
    },
    {
      "metadata": {
        "id": "_2A7PfzkgI3g",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = np.array([i[0] for i in train]).reshape(-1,img_size,img_size,1)\n",
        "Y = [i[1] for i in train]\n",
        "\n",
        "test_x = np.array([i[0] for i in test]).reshape(-1,img_size,img_size,1)\n",
        "test_y = [i[1] for i in test]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OAHsmz51AUH_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Time to fit our data in model we created earlier. I have provided epoch value to 5. And we can see the change in the Loss value and Accuracy with each epoch."
      ]
    },
    {
      "metadata": {
        "id": "47_mr0mFjKnf",
        "colab_type": "code",
        "outputId": "0742acaf-cf77-497c-90ae-8eda38b18605",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit({'input':X},{'targets':Y},n_epoch=5,validation_set=({'input':test_x},{'targets':test_y}),snapshot_step = 500,show_metric=True,run_id=model_name)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "Run id: cars vs truck-0.001-6conv-basic.model\n",
            "Log directory: log/\n",
            "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
            "---------------------------------\n",
            "Training samples: 20\n",
            "Validation samples: 10\n",
            "--\n",
            "Training Step: 1  | time: 1.662s\n",
            "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 | val_loss: 11.51293 - val_acc: 0.5000 -- iter: 20/20\n",
            "--\n",
            "Training Step: 2  | total loss: \u001b[1m\u001b[32m0.90697\u001b[0m\u001b[0m | time: 1.080s\n",
            "| Adam | epoch: 002 | loss: 0.90697 - acc: 0.4500 | val_loss: 9.12740 - val_acc: 0.5000 -- iter: 20/20\n",
            "--\n",
            "Training Step: 3  | total loss: \u001b[1m\u001b[32m9.28938\u001b[0m\u001b[0m | time: 1.073s\n",
            "| Adam | epoch: 003 | loss: 9.28938 - acc: 0.4909 | val_loss: 1.91942 - val_acc: 0.5000 -- iter: 20/20\n",
            "--\n",
            "Training Step: 4  | total loss: \u001b[1m\u001b[32m7.98277\u001b[0m\u001b[0m | time: 1.074s\n",
            "| Adam | epoch: 004 | loss: 7.98277 - acc: 0.4977 | val_loss: 2.28941 - val_acc: 0.5000 -- iter: 20/20\n",
            "--\n",
            "Training Step: 5  | total loss: \u001b[1m\u001b[32m3.52730\u001b[0m\u001b[0m | time: 1.076s\n",
            "| Adam | epoch: 005 | loss: 3.52730 - acc: 0.4993 | val_loss: 1.58039 - val_acc: 0.5000 -- iter: 20/20\n",
            "--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wDULC7MilMWM",
        "colab_type": "code",
        "outputId": "4355611a-076a-441f-c456-2cd124dc2b20",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model.save(model_name)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:/content/cars vs truck-0.001-6conv-basic.model is not in all_model_checkpoint_paths. Manually adding it.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}